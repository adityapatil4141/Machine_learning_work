# -*- coding: utf-8 -*-
"""End-To-End.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YLeHTyLRoDrZkafSdJNEizRJVUM6zCpb
"""

#Importing necessary libraries:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from mlxtend.plotting import plot_decision_regions

# Mounting Google Drive:
from google.colab import drive
drive.mount('/content/drive')

# Loading Dataframe:
df = pd.read_csv('/content/drive/MyDrive/work files /placement.csv')

# Printing first five rows of dataframe:
df.head()

# Removing unwanted columns form dataframe:          using .drop(columns=) function
df = df.drop(columns=['Unnamed: 0'])

# Checking dataframe information:
df.info()

# Checking null values from the dataset:      using isnull().sum()
df.isnull().sum()

# Plotting scatterplot to check data distribution.
# This step helped us to decide which algorithm to use.
# Both blue dots and orange dots are not mixed intensively so we can draw line in between them. so, we use logistic regression.
sns.scatterplot(x=df['cgpa'],y=df['iq'],hue=df['placement'])

# Defining x and y for training data:
x = df.iloc[:,:-1]
y = df.iloc[:,-1]

# train_test_split with test size as 20% (0.2):
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state =42)

# creating an object for StandardScaler:
scaler = StandardScaler()

# Transforming values of x_train.
# Transforming means, by using StandardScaler we scale value in range -1 to 1.
x_train_transformed = scaler.fit_transform(x_train)
x_test_transformed = scaler.transform(x_test)

# Creating an object for logisticregression:
# Fitting newly transformed x_train and old y_train in it:
# Checking our predictions for x_test
clf = LogisticRegression()
clf.fit(x_train_transformed,y_train)
y_pred = clf.predict(x_test_transformed)

# Checking accuracy score
accuracy_score(y_pred,y_test)

#plotting the distribution:
plot_decision_regions(x_train_transformed,y_train.values,clf=clf,legend=2)

"""# Checking accuracy with cross validition score"""

# This part will we covered after some time.

from sklearn.model_selection import cross_val_score

x_transformed = scaler.fit_transform(x)

np.mean(cross_val_score(clf, x,y,cv=10,scoring = 'accuracy'))